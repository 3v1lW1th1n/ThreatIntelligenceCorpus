Happy New Quantum Year!
If you’ve been our faithful reader and your memory has not been damaged by digital amnesia, you may remember that one of the key insights from Kaspersky Security Bulletin 2015 was a forecast that cryptography as a discipline is on the verge of subdual by quantum computing as a result of progress in bringing the latter to reality. To be honest, I personally thought this forecast a bit ahead of its time, especially being in the part of our bulletin called “Predictions for 2016,” but recent headlines changed my mind.
In the space of just a few weeks in late November and December 2016, we learned that Microsoft is hiring top-notch quantum computing scientists and Intel is hinting about its plans to transform silicon chips into quantum processors that can host millions of qubits (quantum bits — units of quantum information). Such processors can be quite useful for building, for example, an AI based on a neural network of quantum computing devices, the proof-of-concept of which has been reported by researchers from Japan’s Tohoku University. And in early January, news arrived that D-Wave, arguably the world’s best known quantum computing pioneer, is open-sourcing quantum computing software.

In other words, quantum computing is evolving faster than I expected. What does this mean for us, the average users? Does it mean, for example, that we’ll be able to go to a store and buy a “qMac” by the end of the year?
Well, not exactly. Apart from D-Wave, it’s really hard to name another university spin-off that has been able to get far on the bumpy road from laboratory to commercialization. Debates are ongoing about how “really quantum” D-Wave’s device is. I won’t go into that in detail, leaving room for you to read through my colleague’s previous post or read this amazing piece.
Apparently quantum computing is not yet a commodity — as computers became in the 1980s and 90s by the efforts of IBM, Apple, Microsoft, and many others. The complexity and price of quantum computing devices make them better analogous to mainframes, which started to emerge much earlier, in the 1950s.
In the middle of the past century, the biggest obstacle to adoption of the new technology wasn’t the hardware itself; it was the ability to take full advantage of the versatility of the new computing paradigm, which required decades of research. More than three decades of technology development were required before the industry could unveil in the late 1970s all of the building blocks necessary for the emergence of personal computers — and another three decades for PCs to become a basis of modern civilization.
History doesn’t repeat itself, but it often rhymes. Although an important step toward widening the community of quantum computing enthusiasts, D’Wave’s opening of the qbsolve to the developer community is not at all like the emergence of Intel’s x86 architecture or IBM’s PC platform. It actually could have rhymed with Alan Turing’s fundamental works of the 1930s, which laid out the basics of “machine cognition” — that is, if it hadn’t come eight months after IBM’s announcement of the IBM Quantum Experience, which, in my personal opinion, does a much better job of explaining what quantum computing is and how it can be used practically.
I must confess, I was so charmed by IBM that I am thinking of asking for trial access to their quantum processor to test if hash-breaking tasks can be performed with it quicker than with an average system’s CPU or GPU. To add more to my admiration, IBM is a company that is going to witness a second major computing paradigm shift within its lifetime. Nevertheless, given the disparity in the amount of available resources, open-sourcing the software is the right direction for D-Wave to go in the wake of intensifying competition in this market.
As we’ve seen from the headlines, Intel is not planning to miss the quantum revolution, and neither is Microsoft. Those old friends from the 1980s actually have a long history of cooperating with researchers exploring superconducting spin qubits. Few details are available about Intel’s plans, but if the company succeeds in adding the spin qubits to the existing silicon chip designs, that’s going to be a game changer in terms of qubit density.
However, it seems that Intel’s quantum chips, as well as D-Wave’s, still need cooling to the temperature of liquid helium (−452 °F and below). That means a smartphone-grade QPU would need to be housed in a mainframe-size facility. In other words, quantum computing power is not yet meant for personal use.
The simplest way to explain the game change in processing power is to make an analogy with parallel computing. Qubit states are a superposition of conventional “0” and “1,” the amount of which is limited only to the resolving power of the system, so it is fair to some extent to say that information stored in qubits is processed simultaneously. Which means that a quantum processing unit will be some orders of magnitude more powerful than traditional CPU.

Well, the analogy is not perfect, given that quantum computation operations are not exactly the same as basic operations used in digital algebra, but it seems that quantum computing scientists will need some time to take full advantage of the new computing paradigm, just as it took decades with digital.
However, the main question is, what should we do with this humongous computing power? It doesn’t appear that we need all of the flops hidden in today’s gadgets to perform our most common user tasks, despite the effort developers have put into their apps to make them as multimedia as possible.
Well, think again. Have you seen a message from your favorite messaging app letting you know that it now encrypts all conversations? Or, perhaps, you’ve heard about cryptocurrencies —Bitcoin being the most known — or about blockchain technology? Yes, I am talking about cryptography and technologies that are built upon it.
With 2016 a record-high in terms of the amount of information leaks, encryption is becoming a necessity, not just in the corporate sector, where it is now enforced with even more strength, but for consumers as well. Encryption and decryption tasks consume a lot of computing power. So does the bitcoin mining process. Other implementations of blockchain technology may perform cryptography functions on specialized nodes with more computing power available to them. In fact, bitcoin mining is already nearly ineffective on casual PCs — that’s why specialized mining farms are built. But such initiatives, such as building a more secure IoT (Internet of Things) upon blockchain, lead me to the conclusion that encryption is going to be ubiquitous.
And guess what? Cryptography is the kind of task for which quantum computers are going to be especially good.
Quantum computing may bring either salvation or doom to this emerging new world. As we said in our Security Bulletin in 2015, cryptography the way it exists today will definitely lead to doom. The thesis that “cryptography is one of the very few fields where adversarial conflict continues to heavily favor the defender” will be strongly contested (to say the least) until effective postquantum cryptography algorithms are introduced.

Those, in turn, may require much more computing power than conventional computers are ready to yield. But, to our salvation, the miniaturization and commoditization of quantum computer devices is also imminent, which means that there will be more computing power available to defend against attackers. And the never-ending game of attackers vs. defenders will continue on a new level.
Apart from our information security discourse, we still have hope that advances in quantum computing will further boost augmented reality, virtual reality, artificial intelligence, and other resource-hungry applications.
To sum up: Quantum computers appear to be inching closer to reality. You still can’t touch one, but it’s good to see that there are computing platforms for quantum computers that you can check for yourself with IBM or D-Wave. That checking requires a certain level of geekiness, so the majority of Earth’s population still have to wait. But with more big names like Intel, IBM, Google, and Microsoft pouring money into the effort, it seems inevitable that we’ll see at least some practical outcome.
